###**Predicting exercise quality - Weight Lifting Exercise Dataset**

####**Main goal of the project**
The main goal of this project is to predict in the most accurate way the variable "classe" of the testing dataset, through a fitting model based on the training dataset.  

####**Reading and clearing the training dataset**
Reading the datasets:  
```{r, echo=FALSE}
setwd("~/Dropbox/Coursera/Corsi/8. Practical Machine learning/GIT_Coursera_MachineLearning_PGA")
```
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(caret)
training=read.csv(file="pml-training.csv", header = TRUE, sep = ",", na.strings = c("", "NA", "#DIV/0!"))
```
The training set is composed by 19622 obs of 160 variables. Let's find where the NAs are concentrated:
```{r}
notNA_count=apply(training, 2, function(x) sum(!is.na(x)))
table(notNA_count)
```
Only 60 variables are without NAs; all the other 100 variables are NA in ~ 98% of observations. Let's define a new training dataset that exclude those variables:  
```{r}
notNA_index=notNA_count==19622
training_final=training[, notNA_index]
```
All that variables describing "when" the action was completed and "who" completed the action are not useful to predict the goodness of the exercise execution, so I exclude them:  
```{r}
training_final=subset(training_final,
                      select = -c(X,
                                  user_name,
                                  raw_timestamp_part_1,
                                  raw_timestamp_part_2,
                                  cvtd_timestamp,
                                  new_window,
                                  num_window))
```

####**Cross validation and model selection**
The final training dataset is quite large **(19622 obs of 53 variables)**, so it is possible to test the goodness of fit of the model using cross validation. In particular I choose **k-fold cross validation, with k=10**.  
The main goal of this project is to fit a model that generates a prediction of the categorical variable "classes" with the highest accuracy possible on the testing set: because of that I choose a **random forest model**.
```{r}
set.seed(1050)
if(!file.exists("model_rf.rda")){
        #Speeding-up the training process: setting processors' computational power to be used in parallel
        library(parallel)
        library(doParallel)
        cluster <- makeCluster(detectCores() - 1)
        registerDoParallel(cluster)
        #setting a cross-validation method with k = 10
        ctrl = trainControl(method = "cv", number = 10, allowParallel = TRUE)
        #training a random forest model
        model_rf=train(form=classe~., data=training_final, trControl=ctrl)
        #return to standard use of processors
        stopCluster(cluster)
        registerDoSEQ()    
} else{ load("model_rf.rda")}
```

####**Accuracy and out-of-sample error estimate**
As you can see below, the **accuracy of the random forest model is quite high (99.6%)**:
```{r}
confusionMatrix.train(model_rf)
```
**The out-of-sample error estimated through the cross validation method is approximately 0.4%**  
Let's see how the accuracy vary across different number of predictors, and which variables are selected as the most important in the random forest algorithm as per the mean decrease in node impurity (related to the Gini index):  
```{r, fig.height=4, fig.width=10, message=FALSE, warning=FALSE}
library(randomForest)
plot(model_rf, main="Accuracy by predictor count")
varImpPlot(model_rf$finalModel, main="Variable importance plot on the 10 most important variables", sort=TRUE, n.var=10)
```

####**Reading and clearing the testing dataset**
Let's clean the testing dataset in the same way of the training set:
```{r}
testing=read.csv(file="pml-testing.csv", header = TRUE, sep = ",", na.strings = c("", "NA", "#DIV/0!"))
testing_final=testing[, notNA_index]
testing_final=subset(testing_final,
                      select = -c(X,
                                  user_name,
                                  raw_timestamp_part_1,
                                  raw_timestamp_part_2,
                                  cvtd_timestamp,
                                  new_window,
                                  num_window))
```

####**Prediction of the classe variable in the testing dataset**
Here's the prediction of the variable "classe" of the training dataset:
```{r}
class_prediction=predict(object = model_rf, newdata = testing_final)
class_prediction
```
